{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " html title:\n",
      "3 California volcanoes are at the top of federal volcano threat list - Los Angeles Times\n",
      "\n",
      " top_n_entities:\n",
      "['siskiyou modoc', 'helens', 'mammoth lakes', 'medicine lake', 'long valley', 'lassen peak', 'elko nev', 'salton buttes imperial county clear lake volcanic field lake county', 'shasta lassen', 'mauna loa', 'mount rainier mt', 'san francisco', 'pacific northwest', 'hawaii big island kilauea', 'salton buttes', 'iceland eyjafjallaj', 'mount hood crater', 'shasta siskiyou county lassen volcanic center shasta county long valley caldera', 'ash', 'eureka sacramento']\n",
      "20\n",
      "\n",
      " most_freq_nouns:\n",
      "[u'volcanic', u'california', u'eruption', u'volcanoes', u'volcano', u'ash', u'lake', u'areas', u'shasta', u'lassen', u'miles', u'county', u'valley', u'mt', u'helens', u'years', u'risk']\n",
      "\n",
      "symbols_numbers_density:\n",
      "[50, 0]\n",
      "\n",
      "short sentences:\n",
      "[Sentence(\"Shasta towns Mount Shasta Weed.\"), Sentence(\"St. Helens erupted .\"), Sentence(\"California volcanoes prolific prehistoric times.\"), Sentence(\"Mt.\"), Sentence(\"St. Helens contrast erupted .\"), Sentence(\"cubic miles material.\"), Sentence(\"St. Helens erupted May .\"), Sentence(\"p.m.\"), Sentence(\"This article originally published noon.\")]\n",
      "\n",
      "numbers:\n",
      "[]\n",
      "\n",
      "emails:\n",
      "[]\n",
      "\n",
      "subjects:\n",
      "['helens', 'lassen peak', 'shasta lassen', 'shasta siskiyou county lassen volcanic center shasta county long valley caldera', 'ash']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "import sys\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "\n",
    "\"\"\"config data\"\"\"\n",
    "url = 'http://www.latimes.com/local/lanow/la-me-ln-volcano-california-20181025-story.html'\n",
    "nouns = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "stop = stopwords.words('english')\n",
    "email = \"\"\"\n",
    "PI team,\n",
    "\n",
    "The Global Advisory Solutions Conference is tomorrow, Friday October 26th at 12:00-2:00pm EST.  \n",
    "This is exclusive virtual conference will give you insights on why are we moving to solutions, \n",
    "how to activate solutions in the market and what the move to solutions means to you.  \n",
    "Please use the link below to register for the event.\n",
    "\n",
    "Regards,\n",
    "Geoff\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def download_document(url):\n",
    "    \"\"\"Extracts title and all text stored in paragraph tags\"\"\"\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    title = soup.find('title').get_text()\n",
    "    document = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
    "    \n",
    "    print \"\\n html title:\"\n",
    "    print title\n",
    "    return document\n",
    "\n",
    "def clean_document(document):\n",
    "    \"\"\"Remove enronious characters, whitespace and stop words\"\"\"\n",
    "    document = re.sub('[^A-Za-z .-]+', ' ', document)\n",
    "    document = ' '.join(document.split())\n",
    "    document = ' '.join([i for i in document.split() if i not in stop])\n",
    "    return document\n",
    "\n",
    "def tokenize_sentences(document):\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    return sentences\n",
    "\n",
    "def word_freq_dist(document):\n",
    "    \"\"\"Returns a word count frequency distribution\"\"\"\n",
    "    words = nltk.tokenize.word_tokenize(document)\n",
    "    words = [word.lower() for word in words if word not in stop]\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    return fdist\n",
    "\n",
    "def get_phone_numbers(string):\n",
    "    r = re.compile(r'(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})')\n",
    "    phone_numbers = r.findall(string)\n",
    "    return [re.sub(r'\\D', '', number) for number in phone_numbers]\n",
    "\n",
    "def get_email_addresses(string):\n",
    "    r = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "    return r.findall(string)\n",
    "\n",
    "def get_entities(document,entity_label):\n",
    "    \"\"\"Returns Named Entities using NLTK Chunking\"\"\"\n",
    "    entities = []\n",
    "    sentences = tokenize_sentences(document)\n",
    "\n",
    "    # Part of Speech Tagging\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    for tagged_sentence in sentences:\n",
    "        for chunk in nltk.ne_chunk(tagged_sentence):\n",
    "            \n",
    "            # If a chunk has been classified as a named entity, it will be of type nltk.tree.Tree\n",
    "            if type(chunk) == nltk.tree.Tree:\n",
    "                if chunk.label() == entity_label:\n",
    "                    entities.append(' '.join([c[0] for c in chunk]).lower())\n",
    "    return entities\n",
    "\n",
    "def get_subject(document,n):\n",
    "    \"\"\"NER + Frequent Nouns to give subject\"\"\"\n",
    "    # labels: PERSON, LOCATION, ORGANIZATION, MISC, MONEY, NUMBER, ORDINAL, PERCENT, DATE, TIME, DURATION, SET\n",
    "    entity_label = 'PERSON'\n",
    "    \n",
    "    # Get n most frequent Nouns\n",
    "    fdist = word_freq_dist(document)\n",
    "    most_freq_nouns = [w for w, c in fdist.most_common(n)\n",
    "                       if nltk.pos_tag([w])[0][1] in nouns]\n",
    "\n",
    "    # Get Top n name entities\n",
    "    entities = get_entities(str(document), entity_label)\n",
    "    top_n_entities = [w for w, c in nltk.FreqDist(entities).most_common(n)]\n",
    "\n",
    "    # Get the subject noun by looking at the intersection of top n entities\n",
    "    # and most frequent nouns.\n",
    "    subject_nouns = [entity for entity in top_n_entities\n",
    "                    if entity.split()[0] in most_freq_nouns]\n",
    "    \n",
    "    print \"\\n top_n_entities:\"\n",
    "    print top_n_entities\n",
    "    print \"\\n most_freq_nouns:\"\n",
    "    print most_freq_nouns\n",
    "    \n",
    "    return subject_nouns\n",
    "\n",
    "def get_symbols_numbers_density(document):\n",
    "    symbols = set(string.punctuation)\n",
    "    num_symbols = sum([1 for elem in document if elem in symbols])\n",
    "    num_numbers = 0\n",
    "    for elem in document:\n",
    "        num_numbers += sum([c.isdigit() for c in elem])\n",
    "    print [num_symbols, num_numbers]\n",
    "\n",
    "\n",
    "def get_short_sentences(document, sentence_size):\n",
    "    blob = TextBlob(str(document))\n",
    "    short_sentences = list()\n",
    "    for sentence in blob.sentences:\n",
    "          if len(sentence.words) <= sentence_size:\n",
    "            short_sentences.append(sentence.replace(\"\\n\", \" \"))\n",
    "\n",
    "    # for item in random.sample(short_sentences, 10):\n",
    "    print short_sentences\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    document = download_document(url)\n",
    "    top_n_threshold = 20\n",
    "    sentence_size = 7\n",
    "    \n",
    "    #print document\n",
    "    document = clean_document(document)\n",
    "    numbers = get_phone_numbers(str(document))\n",
    "    emails = get_email_addresses(str(document))\n",
    "    subject = get_subject(document,top_n_threshold)\n",
    "    \n",
    "    print\"\\nsymbols_numbers_density:\"\n",
    "    get_symbols_numbers_density(str(document))\n",
    "    print\"\\nshort sentences:\"\n",
    "    get_short_sentences(document, sentence_size)\n",
    "    print\"\\nnumbers:\"\n",
    "    print numbers\n",
    "    print\"\\nemails:\"\n",
    "    print emails\n",
    "    print\"\\nsubjects:\"\n",
    "    print subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textblob.blob.Sentence"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def symbols_and_numbers_density(mentions):\n",
    "    symbols = set(string.punctuation)\n",
    "    num_symbols = sum([1 for elem in mentions if elem in symbols])\n",
    "    num_numbers = 0\n",
    "    for elem in mentions:\n",
    "        num_numbers += sum([c.isdigit() for c in elem])\n",
    "    print [num_symbols, num_numbers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 9]\n"
     ]
    }
   ],
   "source": [
    "symbols_and_numbers_density(str(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
